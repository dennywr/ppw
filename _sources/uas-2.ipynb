{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKkniFt7m1JjHhWLHWAFVm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# UAS"],"metadata":{"id":"Bg3lR1TG4lgu"}},{"cell_type":"markdown","source":["## Crawling Portal Berita Antaranews"],"metadata":{"id":"L7v4cfl64xtK"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","import random"],"metadata":{"id":"POnizqnl4oFc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- request: Library ini memungkinkan Anda untuk mengirim HTTP requests (seperti GET, POST, PUT, DELETE) dan menerima respons dari server. Dengan menggunakan Requests, Anda dapat mengakses dan mengambil data dari URL, mengirim data form, dan berinteraksi dengan API.\n","\n","- beautifulSoup: Beautiful Soup adalah library Python yang digunakan untuk mengambil data dari HTML dan XML. Ini menyediakan cara yang mudah untuk menjelajahi, mencari, dan memanipulasi struktur data dokumen web. Beautiful Soup bekerja dengan parser HTML, membantu Anda mengekstrak informasi dari halaman web dengan lebih mudah.\n","\n","- csv: Modul CSV di Python menyediakan fungsi untuk membaca dan menulis file CSV. Ini memungkinkan Anda untuk dengan mudah bekerja dengan data dalam format CSV, yang umumnya digunakan untuk menyimpan data tabel, terutama dalam konteks analisis data dan pertukaran data antar program.\n","\n","- random: Library ini menyediakan fungsi-fungsi untuk menghasilkan angka acak. Dalam konteks web scraping atau pengujian, Anda mungkin menggunakan library ini untuk memilih elemen acak dari daftar atau untuk menyimulasikan perilaku yang tidak terduga.\n","\n"],"metadata":{"id":"bZJJRHxK41Tv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"omb5ktZlt0O8"},"outputs":[],"source":["# masukkan url\n","response = requests.get(\"https://www.antaranews.com/\")\n","# Isi teks dari respons HTTP yang diterima dari server web setelah melakukan permintaan GET.\n","soup = BeautifulSoup(response.text, 'html.parser')\n","# menemukan semua list yang berisi link kategori\n","first_page = soup.findAll('li',\"dropdown mega-full menu-color1\")\n","\n","# menyimpan kategori\n","save_categori = []\n","for links in first_page:\n","  categori = links.find('a').get('href')\n","  save_categori.append(categori)\n","# save_categori\n","\n","# categori yang akan disearch terdapat pada indeks 1 (politik)\n","categori_search = [save_categori[1]]\n","categori_search"]},{"cell_type":"code","source":["# Inisialisasi list untuk menyimpan data berita\n","datas = []\n","\n","# Iterasi melalui halaman berita\n","for ipages in range(1, 3):\n","\n","    # Iterasi melalui setiap kategori berita\n","    for beritas in categori_search:\n","        # Permintaan untuk halaman berita\n","        response_berita = requests.get(beritas + \"/\" + str(ipages))\n","        namecategori = beritas.split(\"/\")\n","\n","        # Parsing halaman berita dengan BeautifulSoup\n","        soup_berita = BeautifulSoup(response_berita.text, 'html.parser')\n","        pages_berita = soup_berita.findAll('article', {'class': 'simple-post simple-big clearfix'})\n","\n","        # Iterasi melalui setiap artikel dalam halaman berita\n","        for items in pages_berita:\n","            # Mendapatkan link artikel\n","            get_link_in = items.find(\"a\").get(\"href\")\n","\n","            # Request untuk halaman artikel\n","            response_artikel = requests.get(get_link_in)\n","            soup_artikel = BeautifulSoup(response_artikel.text, 'html.parser')\n","\n","            # Ekstraksi informasi dari halaman artikel\n","            judul = soup_artikel.find(\"h1\", \"post-title\").text if soup_artikel.findAll(\"h1\", \"post-title\") else \"\"\n","            label = namecategori[-1]\n","            date = soup_artikel.find(\"span\", \"article-date\").text if soup_artikel.find(\"span\", \"article-date\") else \"Data tanggal tidak ditemukan\"\n","\n","            trash1 = \"\"\n","            cek_baca_juga = soup_artikel.findAll(\"span\", \"baca-juga\")\n","            if cek_baca_juga:\n","                for bacas in cek_baca_juga:\n","                    text_trash = bacas.text\n","                    trash1 += text_trash + ' '\n","\n","            artikels = soup_artikel.find_all('div', {'class': 'post-content clearfix'})\n","            artikel_content = artikels[0].text if artikels else \"\"\n","            artikel = artikel_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(trash1, \"\").replace(\"\\xa0\", \"\")\n","\n","            author = soup_artikel.find(\"p\", \"text-muted small mt10\").text.replace(\"\\t\\t\", \"\") if soup_artikel.findAll(\"p\", \"text-muted small mt10\") else \"\"\n","\n","            # Menambahkan data artikel ke dalam list\n","            datas.append({'Tanggal': date, 'Penulis': author, 'Judul': judul, 'Artikel': artikel, 'Label': label})"],"metadata":{"id":"Zaxk5IoY46E-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterasi melalui halaman berita\n","for ipages in range(1, 3):\n","    # Mengecek apakah iterasi saat ini kurang dari 2\n","    if ipages < 2:\n","        # Menampilkan jumlah data yang telah berhasil di-crawling\n","        print(f'Data berhasil dicrawling sebanyak : {len(datas)}')"],"metadata":{"id":"ucDXkkT548BV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Menyimpan data dalam bentuk CSV\n","csv_filename = 'berita_politik_antaranews.csv'\n","# Membuka file CSV untuk ditulis ('w' mode) dengan encoding utf-8\n","with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n","    # Menentukan kolom (fieldnames) untuk file CSV\n","    fieldnames = ['Tanggal', 'Penulis', 'Judul', 'Artikel', 'Label']\n","    # Membuat objek DictWriter untuk menulis data ke file CSV\n","    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","    # Menulis header (baris pertama) ke file CSV\n","    csv_writer.writeheader()\n","    # Iterasi melalui data yang akan disimpan (diasumsikan datas sudah didefinisikan sebelumnya)\n","    for entry in datas:\n","        # Menulis setiap baris data ke file CSV\n","        csv_writer.writerow(entry)\n","# Menampilkan pesan bahwa data telah disimpan dalam file CSV\n","print(f\"Data telah disimpan dalam file {csv_filename}\")"],"metadata":{"id":"6WUc2kAe4-Qr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import library/package yang dibutuhkan (Memproses Data)"],"metadata":{"id":"DfNjWaFz4_uK"}},{"cell_type":"code","source":["import pandas as pd\n","import re"],"metadata":{"id":"42M48vRV5Ck6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Pandas adalah library open-source yang ditulis dalam bahasa pemrograman Python untuk analisis data dan manipulasi data. Dikembangkan oleh Wes McKinney, Pandas menyediakan struktur data yang efisien dan mudah digunakan untuk memanipulasi dan menganalisis data numerik dan tabular. Pandas sangat populer di kalangan data scientist, analis data, dan pengembang perangkat lunak karena kemudahan penggunaannya dan dukungan yang luas dalam ekosistem Python\n","\n","- re adalah library Python yang menyediakan dukungan untuk ekspresi reguler (regular expressions). Ekspresi reguler adalah urutan karakter yang membentuk pola pencarian. Modul re memungkinkan pencocokkan pola dengan string dan melakukan berbagai operasi seperti pencarian, pencocokan, dan manipulasi string berdasarkan pola tertentu."],"metadata":{"id":"j-12HupN5Dca"}}]}